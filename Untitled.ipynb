{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Conv2D\n",
    "from keras.models import load_model, Model\n",
    "from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes\n",
    "from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):\n",
    "    \"\"\"Filters YOLO boxes by thresholding on object and class confidence.\n",
    "    \n",
    "    Arguments:\n",
    "    box_confidence -- tensor of shape (19, 19, 5, 1)\n",
    "    boxes -- tensor of shape (19, 19, 5, 4)\n",
    "    box_class_probs -- tensor of shape (19, 19, 5, 80)\n",
    "    threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n",
    "    \n",
    "    Returns:\n",
    "    scores -- tensor of shape (None,), containing the class probability score for selected boxes\n",
    "    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes\n",
    "    classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes\n",
    "    \n",
    "    Note: \"None\" is here because you don't know the exact number of selected boxes, as it depends on the threshold. \n",
    "    For example, the actual output size of scores would be (10,) if there are 10 boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute box scores\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    \n",
    "    # find the box_classes \n",
    "    box_classes = K.argmax(box_scores, -1)\n",
    "    box_class_scores = K.max(box_scores, -1)\n",
    "   \n",
    "    # filtering mask based on \"box_class_scores\" by using \"threshold\"\n",
    "    filtering_mask = box_class_scores >= threshold\n",
    "    \n",
    "    # apply the mask to scores, boxes and classes\n",
    "    scores = tf.boolean_mask(box_class_scores, filtering_mask)\n",
    "    boxes = tf.boolean_mask(boxes, filtering_mask)\n",
    "    classes = tf.boolean_mask(box_classes, filtering_mask)\n",
    "    \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Applies Non-max suppression (NMS) to set of boxes\n",
    "    \n",
    "    Arguments:\n",
    "    scores -- tensor of shape (None,), output of yolo_filter_boxes()\n",
    "    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)\n",
    "    classes -- tensor of shape (None,), output of yolo_filter_boxes()\n",
    "    max_boxes -- integer, maximum number of predicted boxes you'd like\n",
    "    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n",
    "    \n",
    "    Returns:\n",
    "    scores -- tensor of shape (, None), predicted score for each box\n",
    "    boxes -- tensor of shape (4, None), predicted box coordinates\n",
    "    classes -- tensor of shape (, None), predicted class for each box\n",
    "    \n",
    "    Note: The \"None\" dimension of the output tensors has obviously to be less than max_boxes. Note also that this\n",
    "    function will transpose the shapes of scores, boxes, classes. This is made for convenience.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')     # tensor to be used in tf.image.non_max_suppression()\n",
    "    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor\n",
    "    \n",
    "    # list of indices corresponding to boxes to keep\n",
    "    nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes, iou_threshold)\n",
    "    \n",
    "    # select only nms_indices from scores, boxes and classes\n",
    "    scores = K.gather(scores, nms_indices)\n",
    "    boxes = K.gather(boxes, nms_indices)\n",
    "    classes = K.gather(classes, nms_indices)\n",
    "        \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_eval(yolo_outputs, image_shape, max_boxes=10, score_threshold=.6, iou_threshold=.5):\n",
    "    \"\"\"\n",
    "    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates \n",
    "    and classes.\n",
    "    \n",
    "    Arguments:\n",
    "    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:\n",
    "                    box_confidence: tensor of shape (None, 19, 19, 5, 1)\n",
    "                    box_xy: tensor of shape (None, 19, 19, 5, 2)\n",
    "                    box_wh: tensor of shape (None, 19, 19, 5, 2)\n",
    "                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)\n",
    "    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)\n",
    "    max_boxes -- integer, maximum number of predicted boxes you'd like\n",
    "    score_threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n",
    "    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n",
    "    \n",
    "    Returns:\n",
    "    scores -- tensor of shape (None, ), predicted score for each box\n",
    "    boxes -- tensor of shape (None, 4), predicted box coordinates\n",
    "    classes -- tensor of shape (None,), predicted class for each box\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve outputs of the YOLO model \n",
    "    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs\n",
    "\n",
    "    # convert boxes for filtering functions \n",
    "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
    "\n",
    "    # perform score-filtering with a threshold of score_threshold \n",
    "    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = score_threshold)\n",
    "    \n",
    "    # scale boxes back to original image shape.\n",
    "    boxes = scale_boxes(boxes, image_shape)\n",
    "\n",
    "    # perform Non-max suppression with a threshold of iou_threshold \n",
    "    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold)\n",
    "    \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = read_classes(\"model_data/coco_classes.txt\")\n",
    "anchors = read_anchors(\"model_data/yolo_anchors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anup\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "yolo_model = load_model(\"model_data/y1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp(sess):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        out_scores, out_boxes, out_classes = predict(sess, cap, frame)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sess, cap, image_file):\n",
    "    \"\"\"\n",
    "    Runs the graph stored in \"sess\" to predict boxes for \"image_file\". Prints and plots the preditions.\n",
    "    \n",
    "    Arguments:\n",
    "    sess -- your tensorflow/Keras session containing the YOLO graph\n",
    "    image_file -- name of an image stored in the \"images\" folder.\n",
    "    \n",
    "    Returns:\n",
    "    out_scores -- tensor of shape (None, ), scores of the predicted boxes\n",
    "    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes\n",
    "    out_classes -- tensor of shape (None, ), class index of the predicted boxes\n",
    "    \n",
    "    Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes. \n",
    "    \"\"\"\n",
    "    # getting test image size   \n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    width = np.array(width, dtype=float)\n",
    "    height = np.array(height, dtype=float)\n",
    "    image_shape = (height, width)\n",
    "    \n",
    "    scores, boxes, classes = yolo_eval(yolo_outputs, image_shape)    \n",
    "    \n",
    "    # preprocessing the image\n",
    "    # image, image_data = preprocess_image(image_file, model_image_size = (608, 608))\n",
    "    resize = cv2.resize(image_file, (608, 608))\n",
    "    image_data = np.array(resize, dtype='float32')\n",
    "    image_data /= 255.\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "\n",
    "    # Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n",
    "    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes],feed_dict={yolo_model.input:image_data,K.learning_phase(): 0})\n",
    "    \n",
    "    # print predictions info\n",
    "    print('Found {} boxes for {}'.format(len(out_boxes), image_file))\n",
    "    # generate colors for drawing bounding boxes.\n",
    "    colors = generate_colors(class_names)\n",
    "    # draw bounding boxes on the image file\n",
    "    draw_boxes(image_file, out_scores, out_boxes, out_classes, class_names, colors)\n",
    "    \n",
    "    return out_scores, out_boxes, out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 boxes for [[[  0 183 255]\n",
      "  [ 57   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [182 179 175]\n",
      "  [179 176 185]\n",
      "  [187 184 193]]\n",
      "\n",
      " [[ 82 111 117]\n",
      "  [ 82 111 117]\n",
      "  [ 82 113 119]\n",
      "  ...\n",
      "  [180 178 178]\n",
      "  [172 176 184]\n",
      "  [184 188 196]]\n",
      "\n",
      " [[ 81 112 118]\n",
      "  [ 84 115 121]\n",
      "  [ 75 124 111]\n",
      "  ...\n",
      "  [178 176 175]\n",
      "  [170 174 182]\n",
      "  [183 187 195]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[152 170 155]\n",
      "  [149 167 152]\n",
      "  [149 167 152]\n",
      "  ...\n",
      "  [188 197 197]\n",
      "  [185 200 202]\n",
      "  [193 208 210]]\n",
      "\n",
      " [[146 171 155]\n",
      "  [144 169 153]\n",
      "  [146 169 153]\n",
      "  ...\n",
      "  [192 201 200]\n",
      "  [189 205 197]\n",
      "  [197 213 205]]\n",
      "\n",
      " [[150 168 153]\n",
      "  [149 167 152]\n",
      "  [150 168 153]\n",
      "  ...\n",
      "  [187 201 199]\n",
      "  [182 197 212]\n",
      "  [189 204 219]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-4f55b4c8e2bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-1f12137c5f96>\u001b[0m in \u001b[0;36mimp\u001b[1;34m(sess)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mout_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Frame\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-ae621631801a>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(sess, cap, image_file)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# draw bounding boxes on the image file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mdraw_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\gHUB\\Object Detection\\yolo_utils.py\u001b[0m in \u001b[0;36mdraw_boxes\u001b[1;34m(image, out_scores, out_boxes, out_classes, class_names, colors)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'font/FiraMono-Medium.otf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3e-2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mthickness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "imp(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n",
      "640.0 480.0\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    width = vcap.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
    "    height = vcap.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
    "    print(width, height)\n",
    "    print(type(width))\n",
    "    # Display the resulting frame\n",
    "    cv.imshow('frame',gray)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"images/check1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n"
     ]
    }
   ],
   "source": [
    "print(image.size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "thickness = (image.size[0] + image.size[1]) // 300\n",
    "print(thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
